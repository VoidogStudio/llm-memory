# ADR-001: v1.1.0 Feature Architecture

**Status:** Accepted
**Date:** 2025-12-06
**Version:** 1.1.0
**Related Issues:** FR-001, FR-002, FR-003, FR-004

## Context

llm-memory v1.1.0 introduces four major features to enhance memory management capabilities:
1. Memory Consolidation with extractive summarization
2. Importance Scoring based on access patterns
3. Hybrid Search combining FTS5 and semantic search
4. Batch Operations for bulk store/update

## Decision

Implement a **3-tier Layered Architecture** with the following components:

### Layer 1: Service Layer (Business Logic)
- **ConsolidationService**: Memory grouping and summarization
- **ImportanceService**: Importance score calculation and management
- **TokenizationService**: Japanese morphological analysis (optional)
- **MemoryService** (extended): Batch operations

### Layer 2: Repository Layer (Data Access)
- **MemoryRepository** (extended): Keyword search, hybrid search, access logging

### Layer 3: Tool Layer (MCP Interface)
- **batch_tools**: memory_batch_store, memory_batch_update
- **importance_tools**: memory_get_score, memory_set_score
- **consolidation_tools**: memory_consolidate
- **memory_tools** (extended): Enhanced search with search_mode parameter

## Database Schema Changes (v1 → v2)

### New Tables
- **memories_fts**: Full-Text Search (FTS5) virtual table
- **memory_access_log**: Access tracking for importance scoring

### New Columns (memories table)
- `importance_score`: FLOAT (default 0.5)
- `access_count`: INTEGER (default 0)
- `last_accessed_at`: DATETIME
- `consolidated_from`: TEXT (tracks consolidation ancestry)

## Implementation Details

### FR-001: Memory Consolidation
**Service:** ConsolidationService
**Tools:** memory_consolidate

- Groups multiple memories by semantic similarity
- Uses extractive summarization to create consolidated memory
- Supports optional Japanese tokenization via SudachiPy
- Options: preserve_originals (default: true)
- Constraints:
  - Min memories: 2, Max: 50
  - Max summary length: 4000 chars

**Summarization Algorithm:**
1. Split content into sentences
2. Calculate word frequency scores
3. Score each sentence based on word frequencies
4. Select top sentences maintaining order

### FR-002: Importance Scoring
**Service:** ImportanceService
**Tools:** memory_get_score, memory_set_score
**Repository Methods:** log_access, update_importance, get_access_stats

- Calculates importance based on:
  - Access frequency (access_count)
  - Recency (time since last access)
  - Age of memory (older memories age gracefully)
- Range: [0.0, 1.0]
- Access logging triggered on: get, search, batch_get operations
- Cleanup: Retain last 30 days of access logs

**Score Calculation:**
```
score = (0.4 × access_frequency) + (0.3 × recency) + (0.3 × age_factor)
```

### FR-003: Hybrid Search
**Service:** MemoryService.search (extended)
**Repository:** keyword_search, hybrid_search
**Utility:** RRF (Reciprocal Rank Fusion)

- Combines two search strategies:
  1. Keyword search via FTS5
  2. Semantic search via embeddings
- Ranking: Reciprocal Rank Fusion (RRF)
- Parameters:
  - `search_mode`: "keyword" | "semantic" | "hybrid"
  - `keyword_weight`: [0.0, 1.0] (default 0.5)
  - `sort_by`: "relevance" | "importance" | "recency"
  - `importance_weight`: [0.0, 1.0] (default 0.3)

**RRF Formula:**
```
score = Σ(1 / (k + rank_i)) for each search result
```

### FR-004: Batch Operations
**Service:** MemoryService
**Tools:** memory_batch_store, memory_batch_update

- Bulk store up to 100 memories per call
- Bulk update with optional rollback mode
- Error handling:
  - Mode 1 (rollback): All-or-nothing, transaction rolls back on any error
  - Mode 2 (skip): Continues on error, returns success/error counts
- Returns: {success_count, error_count, created_ids, errors}
- Max batch size: 100

## Implementation Order

1. **Priority 1 (FR-004):** Batch Operations - Foundation for bulk operations
2. **Priority 2 (FR-002):** Importance Scoring - Core scoring system
3. **Priority 3 (FR-003):** Hybrid Search - Builds on importance scoring
4. **Priority 4 (FR-001):** Consolidation - Uses all other features

## Test Coverage

| Feature | Test Suite | Tests | Coverage |
|---------|-----------|-------|----------|
| FR-001 | test_consolidation | 10 | 100% |
| FR-002 | test_importance | 8 | 100% |
| FR-003 | test_hybrid_search | 10 | 100% |
| FR-004 | test_batch_operations | 9 | 100% |
| Migration | test_migration | 8 | 100% |
| Performance | test_performance | 9 | 100% |
| **Total** | **6 suites** | **54** | **100%** |

## Performance Targets

| Operation | Target |
|-----------|--------|
| Batch store (100 items) | < 5 seconds |
| Hybrid search (10k memories) | < 500 ms |
| Consolidation (50 memories) | < 3 seconds |
| Migration (100k memories) | < 30 seconds |

## Dependencies

**Required:** None (v1.1.0 uses only stdlib and existing deps)

**Optional:**
- `SudachiPy >= 0.6.8`: Japanese morphological analysis
- `SudachiDict >= 20250129`: Dictionary data for SudachiPy

## Migration Path

Automatic v1 → v2 migration on first `Database.initialize()` call:
1. Create memories_fts table with FTS5
2. Create memory_access_log table
3. Add new columns to memories (backwards compatible, default values)
4. Populate FTS5 index from existing memories
5. No downtime required

## Consequences

### Positive
- Rich memory management capabilities
- Flexible search combining keyword and semantic approaches
- Access-based importance scoring
- Memory consolidation reduces storage and improves search relevance

### Negative
- Increased database complexity (3 new tables/indexes)
- Slight performance overhead for access logging on get()
- Optional SudachiPy dependency adds 10+ MB to package

### Neutral
- Japanese tokenization optional (graceful fallback to Unicode)

## Alternatives Considered

1. **Importance without access logging:** Simpler but can't track recency
2. **Keyword-only or semantic-only search:** Less flexible, lower relevance
3. **Manual consolidation via API:** Less intelligent grouping
4. **Synchronous batch operations:** Blocks on large batches

## Related Resources

- Design Doc: `.tmp/order/04_design/detailed_design.md`
- Issue Analysis: `.tmp/order/08_debug/fix_instructions.json`
- Test Suites: `tests/test_*.py`
